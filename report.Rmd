---
title: "Linear models for Gaussian data"
subtitle: "TMA4315 - Exercise 1"
author: Anders Christiansen SÃ¸rby, Edvard Hove
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("car")
#install.packages("GGally")
library(car)
library(GGally)
library(mylm)
```

We will work on the dataset `carData`, which consists of 3987 observations on the following 5 variables:

* `wages`, composite hourly wage rate from all jobs
* `education`, number of years in schooling
* `age`, in years
* `sex`, Male or Female
* `language`, English, French or Other

```{r cars, echo=FALSE, message=FALSE}
data(SLID, package = "carData")
SLID <- SLID[complete.cases(SLID), ]
n <- length(SLID[,1])
```

Using the function `ggpairs` we get the following diagnostic plot matrix:

```{r pressure, echo=FALSE, message=FALSE, warning=FALSE}
ggpSLID = ggpairs(SLID)
print(ggpSLID,progress=F)
```
The plot suggests that `wages` increase with both `education` and `age`, despite the apparent slight decrease of `education` with `age`. It also seems like males recieve higher wages than females, despite similar levels of age and education. There seems to be some relationship between `language` and `age`, but it is not clear if there is a corresponding relationship between `language` and `wages`. 

When performing a multiple linear regression analysis to study how `wages` depends on the explanatory variables we need to assume that: ELABORATE

- each data point \(Y_i\) is independent
- the residuals are normally distributed with a homogenous variance, such that \(Y_i \sim \mathcal{N}(\mu_i,\sigma^2)\)
- there is a linear relationship between `wages` and the explanatory variables, such that \(\mu_i = \eta_i = \mathbf{x_i}^T\beta\)


# Part 2: Simple linear regression with the mylm package

**a)**

```{r model1}
model1 <- mylm(wages ~education, data = SLID)
print(model1)

model1b <- lm(wages ~ education, data = SLID)
print(model1b)
```

**b)**

```{r summary}
summary(model1)
#summary(model1b)
```

The error of the regression can be represented using the following quantities. This holds for linear regression.

\begin{align}
SSE = \sum^{n}_{i=1}(\hat{y}_i - \bar{y})^2 \\
SSR = \sum^{n}_{i=1}(y_i - \hat{y}_i)^2 \\
SST = \sum^{n}_{i=1}(y_i - \bar{y}_i)^2 = SSE + SSR \\
R^2 = 1 - \frac{SSR}{SST}
\end{align}
where $\bar{y}$ is the mean of responses and $\hat{y}_i$ is the fitted value. The coefficient of determination, \(R^2\), shows how much of the variance of the prediction can be explained by the variance of the response.

**c)**

**d)**

The residual sum of squares (SSE) for this model is \(`r sprintf('%.3f', model1$sse)`\). 
The model has \(`r n-length(model1$beta)`\) degrees of freedom, which is obtained by subtracting the number of estimated parameters from the number of observations.
The total sum of squares (SST) is \(`r sprintf('%.3f', model1$sst)`\). Assuming $\beta_1=0$ we have

\begin{equation}
\frac{}{}
\end{equation}
**e)**
